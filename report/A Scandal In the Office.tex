
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{A Scandal In the Office}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    \pagenumbering{gobble}
    \input{kapak.tex}
    \newpage

    \pagenumbering{roman}
    \tableofcontents
    \newpage
    
    \pagenumbering{arabic}

    \hypertarget{intro}{%
\section{Introduction}\label{intro}}

    In this assignment, we are asked to classify occupancy status in an office room\footnote{Original paper: http://www.sciencedirect.com/science/article/pii/S0378778815304357}. There are three data sets. One for training and two for testing. The second test set consists of the data recorded mostly when the door was open. This assignment is done with Python using sci-kit learn machine learning library.


    \hypertarget{investigating-the-data-and-exploratory-data-analysis}{%
\section{Investigating the Data and Exploratory Data
Analysis}\label{investigating-the-data-and-exploratory-data-analysis}}

    In this section, I have investigated the data, presented some
visualizations and analysed features.

Firstly I will import necessary Python modules and read the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{datetime}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{Imputer}\PY{p}{,} \PY{n}{StandardScaler}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{classification\PYZus{}report}\PY{p}{,} \PY{n}{confusion\PYZus{}matrix}
\end{Verbatim}


    Reading the data into Pandas DataFrames as train, test1 and test2:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{datatraining.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{test1} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{datatest.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{test2} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{datatest2.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    Now, to see first few rows of the data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Set 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{test1}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Set 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{test2}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Training Set
                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \textbackslash{}
1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   
2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   
3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   
4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   
5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   

   Occupancy  
1          1  
2          1  
3          1  
4          1  
5          1  

Test Set 1
                    date  Temperature  Humidity       Light         CO2  \textbackslash{}
140  2015-02-02 14:19:00      23.7000    26.272  585.200000  749.200000   
141  2015-02-02 14:19:59      23.7180    26.290  578.400000  760.400000   
142  2015-02-02 14:21:00      23.7300    26.230  572.666667  769.666667   
143  2015-02-02 14:22:00      23.7225    26.125  493.750000  774.750000   
144  2015-02-02 14:23:00      23.7540    26.200  488.600000  779.000000   

     HumidityRatio  Occupancy  
140       0.004764          1  
141       0.004773          1  
142       0.004765          1  
143       0.004744          1  
144       0.004767          1  

Test Set 2
                  date  Temperature   Humidity       Light          CO2  \textbackslash{}
1  2015-02-11 14:48:00      21.7600  31.133333  437.333333  1029.666667   
2  2015-02-11 14:49:00      21.7900  31.000000  437.333333  1000.000000   
3  2015-02-11 14:50:00      21.7675  31.122500  434.000000  1003.750000   
4  2015-02-11 14:51:00      21.7675  31.122500  439.000000  1009.500000   
5  2015-02-11 14:51:59      21.7900  31.133333  437.333333  1005.666667   

   HumidityRatio  Occupancy  
1       0.005021          1  
2       0.005009          1  
3       0.005022          1  
4       0.005022          1  
5       0.005030          1  

    \end{Verbatim}

    After I get the main intuition, I am investigating further to see some
analytical attributes:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{train}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}        Temperature     Humidity        Light          CO2  HumidityRatio  \textbackslash{}
        count  8143.000000  8143.000000  8143.000000  8143.000000    8143.000000   
        mean     20.619084    25.731507   119.519375   606.546243       0.003863   
        std       1.016916     5.531211   194.755805   314.320877       0.000852   
        min      19.000000    16.745000     0.000000   412.750000       0.002674   
        25\%      19.700000    20.200000     0.000000   439.000000       0.003078   
        50\%      20.390000    26.222500     0.000000   453.500000       0.003801   
        75\%      21.390000    30.533333   256.375000   638.833333       0.004352   
        max      23.180000    39.117500  1546.333333  2028.500000       0.006476   
        
                 Occupancy  
        count  8143.000000  
        mean      0.212330  
        std       0.408982  
        min       0.000000  
        25\%       0.000000  
        50\%       0.000000  
        75\%       0.000000  
        max       1.000000  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{test1}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}        Temperature     Humidity        Light          CO2  HumidityRatio  \textbackslash{}
        count  2665.000000  2665.000000  2665.000000  2665.000000    2665.000000   
        mean     21.433876    25.353937   193.227556   717.906470       0.004027   
        std       1.028024     2.436842   250.210906   292.681718       0.000611   
        min      20.200000    22.100000     0.000000   427.500000       0.003303   
        25\%      20.650000    23.260000     0.000000   466.000000       0.003529   
        50\%      20.890000    25.000000     0.000000   580.500000       0.003815   
        75\%      22.356667    26.856667   442.500000   956.333333       0.004532   
        max      24.408333    31.472500  1697.250000  1402.250000       0.005378   
        
                 Occupancy  
        count  2665.000000  
        mean      0.364728  
        std       0.481444  
        min       0.000000  
        25\%       0.000000  
        50\%       0.000000  
        75\%       1.000000  
        max       1.000000  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{test2}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}        Temperature     Humidity        Light          CO2  HumidityRatio  \textbackslash{}
        count  9752.000000  9752.000000  9752.000000  9752.000000    9752.000000   
        mean     21.001768    29.891910   123.067930   753.224832       0.004589   
        std       1.020693     3.952844   208.221275   297.096114       0.000531   
        min      19.500000    21.865000     0.000000   484.666667       0.003275   
        25\%      20.290000    26.642083     0.000000   542.312500       0.004196   
        50\%      20.790000    30.200000     0.000000   639.000000       0.004593   
        75\%      21.533333    32.700000   208.250000   831.125000       0.004998   
        max      24.390000    39.500000  1581.000000  2076.500000       0.005769   
        
                 Occupancy  
        count  9752.000000  
        mean      0.210111  
        std       0.407408  
        min       0.000000  
        25\%       0.000000  
        50\%       0.000000  
        75\%       0.000000  
        max       1.000000  
\end{Verbatim}
            
    And, how many rows and columns are there?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{test1}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{test2}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(8143, 7)
(2665, 7)
(9752, 7)

    \end{Verbatim}

    Well\ldots{} It seems the data set has an unnamed id column which
mismatches with date. Before it causes trouble, I will delete it for
making header and the data fitting each other. Additionally, the second
test set seems to lack quotation marks around the dates.

To fix these, I will read files line by line and then, for the first
line, I will change `date' to `Date' to follow other column names' style
of capitalization. For every other line I will remove the characters
before the first comma. I will do it for all three of the files. For the
second test set, I will surround Date column with quotation marks too.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} For training data set:}
        \PY{n}{lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datatraining.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{lines} \PY{o}{=} \PY{n}{f}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{p}{)}
            
        \PY{n}{new\PYZus{}lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{:}
            \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{l} \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{line}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{f}\PY{o}{.}\PY{n}{writelines}\PY{p}{(}\PY{n}{new\PYZus{}lines}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} For test1 data set:}
        \PY{n}{lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datatest.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{lines} \PY{o}{=} \PY{n}{f}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{p}{)}
            
        \PY{n}{new\PYZus{}lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{:}
            \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{l} \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{line}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test1.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{f}\PY{o}{.}\PY{n}{writelines}\PY{p}{(}\PY{n}{new\PYZus{}lines}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} For test2 data set:}
        \PY{n}{lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datatest2.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{lines} \PY{o}{=} \PY{n}{f}\PY{o}{.}\PY{n}{readlines}\PY{p}{(}\PY{p}{)}
            
        \PY{n}{new\PYZus{}lines} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{:}
            \PY{n}{i} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
            \PY{n}{ii} \PY{o}{=} \PY{n}{line}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{line} \PY{o}{=} \PY{n}{line}\PY{p}{[}\PY{p}{:}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{line}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{n}{ii}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n}{line}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{n}{ii}\PY{p}{:}\PY{p}{]}
            \PY{n}{new\PYZus{}lines}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{l} \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{line}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test2.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{f}\PY{o}{.}\PY{n}{writelines}\PY{p}{(}\PY{n}{new\PYZus{}lines}\PY{p}{)}
\end{Verbatim}


    Re-read data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test1} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test1.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test2} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test2.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Now, without further ado, I will check for null values. If there are
any, I should impute them with suitable values:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Check NaNs for train:}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Check NaNs for test1:}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{test1}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Check NaNs for test2:}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{test2}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Date             0
Temperature      0
Humidity         0
Light            0
CO2              0
HumidityRatio    0
Occupancy        0
dtype: int64

Date             0
Temperature      0
Humidity         0
Light            0
CO2              0
HumidityRatio    0
Occupancy        0
dtype: int64


Date             0
Temperature      0
Humidity         0
Light            0
CO2              0
HumidityRatio    0
Occupancy        0
dtype: int64

    \end{Verbatim}

    There aren't any null values in the sets. So, no need for imputing.

    \hypertarget{one-visualization-to-rule-them-all}{%
\subsection{One Visualization to Rule Them
All\ldots{}}\label{one-visualization-to-rule-them-all}}

    As an exploration on the data, I am now plotting the scatter matrix with
respect to occupancy column:\footnote{About the title, original quote by J.J.R. Tolkien: http://tolkiengateway.net/wiki/File:J.R.R.\_Tolkien\_-\_Ring\_verse.jpg}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    What I conclude from this scenery is that I need light. Humidity ratio
and humidity are highly correlated. Also CO2 and humidity ratio together
are useless. Temperature with CO2 nor humidity (nor humidity ratio) too
do not do well. It seems that light with anything will handle the
situation.

    Now I want to see time series of every feature. To do so, I need to
convert date strings to Python datetime objects. This function should be
handy:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{dateOrNotToDate}\PY{p}{(}\PY{n}{date\PYZus{}str}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}\PY{n}{date\PYZus{}str}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{Y\PYZhy{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{m\PYZhy{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{H:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{M:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    For changing string dates to Python datetime object, I will write a
function that accepts a DataFrame as parameter and iterates through its
rows and replaces the string date to datetime object. After writing this
function, I may be able to run it with all three of the data sets like
following:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{convert\PYZus{}dates}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{date} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{get\PYZus{}loc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{dateOrNotToDate}\PY{p}{(}\PY{n}{date}\PY{p}{)}
         \PY{n}{convert\PYZus{}dates}\PY{p}{(}\PY{n}{train}\PY{p}{)}
         \PY{n}{convert\PYZus{}dates}\PY{p}{(}\PY{n}{test1}\PY{p}{)}
         \PY{n}{convert\PYZus{}dates}\PY{p}{(}\PY{n}{test2}\PY{p}{)}
\end{Verbatim}


    Below, I am plotting every feature in time series:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{}plt.style.use(\PYZsq{}ggplot\PYZsq{})}
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{col} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{train}\PY{p}{[}\PY{n}{col}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{col}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{col}\PY{p}{)}
             \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{xaxis\PYZus{}date}\PY{p}{(}\PY{p}{)}
             \PY{n}{fig}\PY{o}{.}\PY{n}{autofmt\PYZus{}xdate}\PY{p}{(}\PY{p}{)}
             \PY{n}{fig}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{analysing-occupancy}{%
\subsection{Analysing Occupancy}\label{analysing-occupancy}}

    Well\ldots{} A wide gap between 07-09.02.2015. I wonder if those days
are weekend\ldots{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{days} \PY{o}{=} \PY{p}{[}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Monday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tuesday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Wednesday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Thursday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Friday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Saturday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sunday}\PY{l+s+s1}{\PYZsq{}}
         \PY{p}{]}
         \PY{n}{seventh\PYZus{}of\PYZus{}feb} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2015\PYZhy{}02\PYZhy{}07}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{Y\PYZhy{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{m\PYZhy{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{days}\PY{p}{[}\PY{n}{seventh\PYZus{}of\PYZus{}feb}\PY{o}{.}\PY{n}{weekday}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Saturday

    \end{Verbatim}

    Just as I thought. The officers don't visit their place on weekends.

    It would be very good if I had the working hours. If I could get the
start indices of every day in the dates, I could iterate through days
and for every day I could plot occupancy in time series.

    To do so, I will store the Date column in a list, and day start indices
in another. Iterating through 5 to 10, I will get those dates' start
index in the dataset:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{date\PYZus{}list} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{Date}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
         \PY{n}{day\PYZus{}start\PYZus{}indices} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
             \PY{n}{day\PYZus{}start\PYZus{}indices}\PY{o}{.}\PY{n}{append}\PY{p}{(}
                 \PY{n}{date\PYZus{}list}\PY{o}{.}\PY{n}{index}\PY{p}{(}
                     \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}
                         \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2015\PYZhy{}02\PYZhy{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ 00:00:00}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{Y\PYZhy{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{m\PYZhy{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{H:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{M:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{S}\PY{l+s+s1}{\PYZsq{}}
                     \PY{p}{)}
                 \PY{p}{)}
             \PY{p}{)}
         \PY{n}{day\PYZus{}start\PYZus{}indices} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{day\PYZus{}start\PYZus{}indices}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0, 369, 1809, 3249, 4689, 6129, 7569]

    \end{Verbatim}

    So, first 369 rows are from 4th of Feb.~Subsequent rows, from 370 to
1808 are from 5th of Feb.~etc.

    Now, I can readily plot occupancy in time series:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{)}\PY{p}{)}\PY{p}{:}    
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
             \PY{k}{if} \PY{n}{i} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
                     \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                     \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{[}
                         \PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}
                     \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}\PY{p}{]}\PY{p}{,}
                     \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{4}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{th of Feb.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{)}
             \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{xaxis\PYZus{}date}\PY{p}{(}\PY{p}{)}
             \PY{n}{fig}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    So, what I conclude from these plots is I had better mind the working
hours which are --rougly speaking-- between 8am and 6pm. Also there
seems to be a lunch break around 1pm. Let me make timing more concrete.

If I print every first and last occurence of occupancy in every days, I
think I will get an idea of the working hours for these officers:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Daily Work Hours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}\PYZti{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{k}{try}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Start:}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                       \PY{n}{train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZgt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
                                 \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZlt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
                                 \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Occupancy} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{End:}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZgt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
                                 \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZlt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
                                 \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Occupancy} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{k}{except}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Daily Work Hours
\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}

Start:	 2015-02-04 17:51:59
End:	 2015-02-04 18:06:00
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

Start:	 2015-02-05 07:38:00
End:	 2015-02-05 18:04:00
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

Start:	 2015-02-06 07:40:59
End:	 2015-02-06 18:06:00
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

No Occupancy
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

No Occupancy
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

Start:	 2015-02-09 08:44:59
End:	 2015-02-09 18:04:00
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#


    \end{Verbatim}

    It appears to be that, officers do not come to office before 07:30 and
they depart after 18:00.

    \hypertarget{analysing-light}{%
\subsection{Analysing Light}\label{analysing-light}}

    Light seems to be less than 400lx at the weekend. Day light must be
illuminating the room atmost 370lx or so. Light follows the same pattern
with occupancy. Interesting enough, there is a sudden increase in the
lighting at the weekend, possibly on 7th of Feb.~Those spots may be
outliers.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{lighting} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}
             \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZgt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
             \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Date} \PY{o}{\PYZlt{}} \PY{n}{date\PYZus{}list}\PY{p}{[}\PY{n}{day\PYZus{}start\PYZus{}indices}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZam{}}
             \PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{Light} \PY{o}{\PYZgt{}} \PY{l+m+mi}{850}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Light}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{lighting}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} May be a lightning?}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
                     Date        Light
3831  2015-02-07 09:42:00  1546.333333
3832  2015-02-07 09:42:59  1451.750000

    \end{Verbatim}

    (P.S: It was too late when I noticed that I had forgotten to remove
outliers. )

    \hypertarget{analysing-co2}{%
\subsection{Analysing CO2}\label{analysing-co2}}

    CO2 data seems to be very useful, since it also follows occupancy
pattern just as light does. Fluctuations can be seen when there is an
occupant in the office.

    \hypertarget{conclusion-of-analyses}{%
\subsection{Conclusion of Analyses}\label{conclusion-of-analyses}}

    After exploratory analyses, I decided to add Weekend and WorkingHours as
features. To do this, I will, again, write a function to apply the
addition to all three of the data sets. For Weekend, as might be
expected, I will check if the date is ``Saturday or Sunday'' or not. If
so, then Weekend = 1, else Weekend = 0.

For WorkingHours, if time of the day is between 07:30 and 18:00, then
WorkingHours = 1, else WorkingHours = 0.

Firstly I will fill these new columns with 0s. Those which fit the
condition will later take their corresponding values.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{k}{def} \PY{n+nf}{add\PYZus{}features}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weekend}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WorkingHour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
         
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{date} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{p}{(}\PY{n}{days}\PY{p}{[}\PY{n}{date}\PY{o}{.}\PY{n}{weekday}\PY{p}{(}\PY{p}{)}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Saturday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{or}\PYZbs{}
                     \PY{p}{(}\PY{n}{days}\PY{p}{[}\PY{n}{date}\PY{o}{.}\PY{n}{weekday}\PY{p}{(}\PY{p}{)}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sunday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                     \PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{get\PYZus{}loc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weekend}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
         
                 \PY{k}{if} \PY{n}{date}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{07:30}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{H:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o+ow}{and}\PYZbs{}
                     \PY{n}{date}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{18:00}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{H:}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                     \PY{n}{df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{get\PYZus{}loc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WorkingHour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{n}{add\PYZus{}features}\PY{p}{(}\PY{n}{train}\PY{p}{)}
         \PY{n}{add\PYZus{}features}\PY{p}{(}\PY{n}{test1}\PY{p}{)}
         \PY{n}{add\PYZus{}features}\PY{p}{(}\PY{n}{test2}\PY{p}{)}
\end{Verbatim}


    After addition of the two features, I will plot the scatter matrix
again:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    As a result, I think Weekend clearly distinguishes the occupancy. So
does the WorkingHour.

Also Weekend and Light together seems to be seperable while Weekend with
Humidity seems less helpful. Likewise, WorkingHour with CO2 seems very
neat and separable.

    \hypertarget{modeling-training-and-testing}{%
\section{Modeling, Training and
Testing}\label{modeling-training-and-testing}}

    After data analyses, now, I shall extract source and target domains for
modeling.

X\_train includes all columns except Occupancy of train DataFrame.
X\_test1 includes all columns except Occupancy of test1 DataFrame.
X\_test2 includes all columns except Occupancy of test2 DataFrame. And
y\_* variables are the corresponding target Series.

I am also defining a list of tuples for features. I will use them in
testing my models with different feature combinations.

Here, I will present tests for these models:
\begin{itemize}
    \item Logistic Regression
    \item Nave Bayes
    \item K-Nearest Neighbors
    \item Decision Tree
    \item Random Forest
    \item Gradient Boosting Machine
    \item Kernelized Support Vector Machine
\end{itemize}

The general convention I follow for every model is,
\begin{enumerate}
    \item Import necessary modules.
    \item Define hyper parameters space.
    \item For every feature combinations in the list mentioned above (and coded below):
    \begin{itemize}
        \item Make grid search cross-validation.
        \item Fit the model and predict against train, test1, and test2 sets. 
        \item Print classification report.
    \end{itemize}
\end{enumerate}

After every model, I presented classification as a table and my
conclusions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{X\PYZus{}test1} \PY{o}{=} \PY{n}{test1}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y\PYZus{}test1} \PY{o}{=} \PY{n}{test1}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{X\PYZus{}test2} \PY{o}{=} \PY{n}{test2}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y\PYZus{}test2} \PY{o}{=} \PY{n}{test2}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Occupancy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{features\PYZus{}combs\PYZus{}list} \PY{o}{=} \PY{p}{[}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weekend}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WorkingHour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Light}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CO2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WorkingHour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CO2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CO2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Temperature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weekend}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{WorkingHour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Light}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CO2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
             \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weekend}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HumidityRatio}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
         \PY{p}{]}
\end{Verbatim}


    \hypertarget{logistic-regression}{%
\subsection{Logistic Regression}\label{logistic-regression}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{penalty}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{,} \PY{l+m+mf}{1.5}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             
             \PY{n}{logit} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                 \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{logit}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{logit}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{logit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{logit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{logit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l1', 'C': 1\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l2', 'C': 1\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   5 1724]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1638   55]
 [   3  969]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      1.00      7703
          1       0.97      1.00      0.98      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7639   64]
 [  10 2039]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l1', 'C': 1.2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.96      0.98      0.97      6414
          1       0.91      0.86      0.88      1729

avg / total       0.95      0.95      0.95      8143


Train Confusion Matrix:
[[6261  153]
 [ 249 1480]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.96      0.95      1693
          1       0.93      0.89      0.91       972

avg / total       0.93      0.93      0.93      2665


Test1 Confusion Matrix:
[[1625   68]
 [ 111  861]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.92      0.91      0.92      7703
          1       0.68      0.72      0.70      2049

avg / total       0.87      0.87      0.87      9752


Test2 Confusion Matrix:
[[7010  693]
 [ 575 1474]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l1', 'C': 1.5\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.92      0.96      0.94      6414
          1       0.82      0.69      0.75      1729

avg / total       0.90      0.90      0.90      8143


Train Confusion Matrix:
[[6156  258]
 [ 528 1201]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.89      0.92      0.90      1693
          1       0.85      0.81      0.83       972

avg / total       0.88      0.88      0.88      2665


Test1 Confusion Matrix:
[[1552  141]
 [ 185  787]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.89      0.85      0.87      7703
          1       0.52      0.62      0.57      2049

avg / total       0.81      0.80      0.81      9752


Test2 Confusion Matrix:
[[6530 1173]
 [ 780 1269]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l1', 'C': 1.5\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6328   86]
 [   4 1725]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1637   56]
 [   2  970]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.98      0.99      7703
          1       0.95      1.00      0.97      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7586  117]
 [   9 2040]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'random\_state': 0, 'penalty': 'l1', 'C': 1\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      6414
          1       0.00      0.00      0.00      1729

avg / total       0.62      0.79      0.69      8143


Train Confusion Matrix:
[[6414    0]
 [1729    0]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.64      1.00      0.78      1693
          1       0.00      0.00      0.00       972

avg / total       0.40      0.64      0.49      2665


Test1 Confusion Matrix:
[[1693    0]
 [ 972    0]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      7703
          1       0.00      0.00      0.00      2049

avg / total       0.62      0.79      0.70      9752


Test2 Confusion Matrix:
[[7703    0]
 [2049    0]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.39\columnwidth}\raggedright
Hyper Parameters\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Train\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Test1\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Test2\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
Weekend, WorkingHour\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.94\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
Light, CO2\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1, `penalty': `l2'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
WorkingHour, CO2\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1.2, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.95\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.93\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.87\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
CO2, Temperature\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1.5, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.90\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.88\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.81\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
Weekend, WorkingHour, Light, CO2\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1.5, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
Weekend, HumidityRatio\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\raggedright
\{`random\_state': 0, `C': 1, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.62\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.40\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.62\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    My two features alone seem to did a good job by catching 94\% precision
in Test2. Weekend and HumidityRatio together is not a good idea as it
seems.

The best are Light-CO2 and Weekend-WorkingHour-Light-CO2. But I am
afraid my features here are not very much helpful since Light-CO2 did
99\% alone. Altough this accuracy is pleasing, my instincts bother me by
saying they are overfitted.

    \hypertarget{nauxefve-bayes}{%
\subsection{Nave Bayes}\label{nauxefve-bayes}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{GaussianNB}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             
             \PY{n}{nb} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)}
             \PY{n}{nb}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{nb}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{!=} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Train : 0.958491956281
Test1 : 0.986859879651
Test2 : 0.895124646936

('Light', 'CO2')
===================================
Train : 0.983544148348
Test1 : 0.992508903353
Test2 : 0.988210733145

('WorkingHour', 'CO2')
===================================
Train : 0.969175979369
Test1 : 0.984772196979
Test2 : 0.826353923615

('CO2', 'Temperature')
===================================
Train : 0.918334766057
Test1 : 0.955790249294
Test2 : 0.767161979614

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Train : 0.98722829424
Test1 : 0.990666830406
Test2 : 0.979737197593

('Weekend', 'HumidityRatio')
===================================
Train : 0.566007613902
Test1 : 0.792091366818
Test2 : 0.407712145401


    \end{Verbatim}

    \begin{longtable}[]{@{}llll@{}}
\toprule
Features & Train & Test1 & Test2\tabularnewline
\midrule
\endhead
`Weekend', `WorkingHour' & 0.9584 & 0.9868 & 0.8951\tabularnewline
`Light', `CO2' & 0.9835 & 0.9925 & 0.9882\tabularnewline
`WorkingHour', `CO2' & 0.9691 & 0.9847 & 0.8263\tabularnewline
`CO2', `Temperature' & 0.9183 & 0.9557 & 0.7671\tabularnewline
`Weekend', `WorkingHour', `Light', `CO2' & 0.9872 & 0.9906 &
0.9797\tabularnewline
`Weekend', `HumidityRatio' & 0.5660 & 0.7920 & 0.4077\tabularnewline
\bottomrule
\end{longtable}

    I don't know what to do. Everything looks so underfitting.

    \hypertarget{k-nearest-neighbors}{%
\subsection{K-Nearest Neighbors}\label{k-nearest-neighbors}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{import} \PY{n}{KNeighborsClassifier}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
         
             \PY{n}{knn} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{knn}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'n\_neighbors': 1\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'n\_neighbors': 33\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   5 1724]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1637   56]
 [   2  970]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.96      0.98      7703
          1       0.88      1.00      0.93      2049

avg / total       0.97      0.97      0.97      9752


Test2 Confusion Matrix:
[[7422  281]
 [   6 2043]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'n\_neighbors': 1\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.99      0.99      6414
          1       0.98      0.98      0.98      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6378   36]
 [  41 1688]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.90      0.92      0.91      1693
          1       0.86      0.82      0.84       972

avg / total       0.88      0.89      0.88      2665


Test1 Confusion Matrix:
[[1559  134]
 [ 172  800]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.93      0.65      0.76      7703
          1       0.38      0.82      0.52      2049

avg / total       0.82      0.68      0.71      9752


Test2 Confusion Matrix:
[[4992 2711]
 [ 365 1684]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'n\_neighbors': 49\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.96      0.94      0.95      6414
          1       0.80      0.85      0.82      1729

avg / total       0.93      0.92      0.92      8143


Train Confusion Matrix:
[[6045  369]
 [ 256 1473]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.91      0.87      0.89      1693
          1       0.79      0.84      0.81       972

avg / total       0.86      0.86      0.86      2665


Test1 Confusion Matrix:
[[1477  216]
 [ 155  817]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.90      0.69      0.78      7703
          1       0.37      0.70      0.49      2049

avg / total       0.79      0.69      0.72      9752


Test2 Confusion Matrix:
[[5299 2404]
 [ 611 1438]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'n\_neighbors': 33\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   5 1724]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1637   56]
 [   2  970]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.96      0.98      7703
          1       0.88      1.00      0.93      2049

avg / total       0.97      0.97      0.97      9752


Test2 Confusion Matrix:
[[7422  281]
 [   6 2043]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'n\_neighbors': 46\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.92      0.92      0.92      6414
          1       0.71      0.70      0.71      1729

avg / total       0.88      0.88      0.88      8143


Train Confusion Matrix:
[[5923  491]
 [ 511 1218]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.69      0.54      0.61      1693
          1       0.42      0.58      0.49       972

avg / total       0.59      0.55      0.56      2665


Test1 Confusion Matrix:
[[916 777]
 [410 562]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.84      0.72      0.77      7703
          1       0.32      0.50      0.39      2049

avg / total       0.73      0.67      0.69      9752


Test2 Confusion Matrix:
[[5520 2183]
 [1029 1020]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
Features & Neighbors & Train & Test1 & Test2\tabularnewline
\midrule
\endhead
`Weekend', `WorkingHour' & 1 & 0.96 & 0.96 & 0.94\tabularnewline
`Light', `CO2' & 33 & 0.99 & 0.98 & 0.97\tabularnewline
`WorkingHour', `CO2' & 1 & 0.99 & 0.88 & 0.82\tabularnewline
`CO2', `Temperature' & 49 & 0.93 & 0.86 & 0.79\tabularnewline
`Weekend', `WorkingHour', `Light', `CO2' & 33 & 0.99 & 0.98 &
0.97\tabularnewline
`Weekend', `HumidityRatio' & 46 & 0.88 & 0.59 & 0.73\tabularnewline
\bottomrule
\end{longtable}

    Hopefully there seems no overfitting nor underfitting situation. This
table too, making my sadness deeper, shows my new features doesn't help
anything at all. Since Light-CO2 again solely hit a 97\% on Test2 with
33 neighbors. Same as the version with my features added. But
Weekend-WorkingHour alone did not a bad job it appears.

    \hypertarget{decision-tree}{%
\subsection{Decision Tree}\label{decision-tree}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeClassifier}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
         
             \PY{n}{tree} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{tree}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} En baarl sonu iin: print(tree.feature\PYZus{}importances\PYZus{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'max\_depth': 2, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      0.99      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   9 1720]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1639   54]
 [   3  969]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      1.00      7703
          1       0.97      0.99      0.98      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7648   55]
 [  12 2037]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 3, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.98      0.99      6414
          1       0.92      0.98      0.95      1729

avg / total       0.98      0.98      0.98      8143


Train Confusion Matrix:
[[6273  141]
 [  42 1687]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.98      0.96      0.97      1693
          1       0.93      0.97      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1620   73]
 [  32  940]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.73      0.84      7703
          1       0.49      0.98      0.65      2049

avg / total       0.89      0.78      0.80      9752


Test2 Confusion Matrix:
[[5622 2081]
 [  38 2011]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.98      0.92      0.95      6414
          1       0.75      0.92      0.83      1729

avg / total       0.93      0.92      0.92      8143


Train Confusion Matrix:
[[5884  530]
 [ 136 1593]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.95      0.80      0.87      1693
          1       0.73      0.93      0.82       972

avg / total       0.87      0.85      0.85      2665


Test1 Confusion Matrix:
[[1355  338]
 [  65  907]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.91      0.53      0.67      7703
          1       0.31      0.80      0.45      2049

avg / total       0.78      0.59      0.62      9752


Test2 Confusion Matrix:
[[4097 3606]
 [ 418 1631]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      0.99      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   9 1720]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1639   54]
 [   3  969]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      1.00      7703
          1       0.97      0.99      0.98      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7648   55]
 [  12 2037]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      6414
          1       0.00      0.00      0.00      1729

avg / total       0.62      0.79      0.69      8143


Train Confusion Matrix:
[[6414    0]
 [1729    0]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.64      1.00      0.78      1693
          1       0.00      0.00      0.00       972

avg / total       0.40      0.64      0.49      2665


Test1 Confusion Matrix:
[[1693    0]
 [ 972    0]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      7703
          1       0.00      0.00      0.00      2049

avg / total       0.62      0.79      0.70      9752


Test2 Confusion Matrix:
[[7703    0]
 [2049    0]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.29\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.41\columnwidth}\raggedright
Hyper Paramters\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Train\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test2\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`Weekend', `WorkingHour'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 2, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.94\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`WorkingHour', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 3, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.89\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`CO2', `Temperature'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.93\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.87\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.78\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.29\columnwidth}\raggedright
`Weekend', `HumidityRatio'\strut
\end{minipage} & \begin{minipage}[t]{0.41\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.62\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.40\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.62\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    Again, 99\% accuracy is good. But I cannot rest assured that it is not
underfitting. It seems the tree didn't need to grow deeper than 3
levels.

    \hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
         
             \PY{n}{tree} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{tree}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{tree}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print(tree.feature\PYZus{}importances\PYZus{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'max\_depth': 2, 'random\_state': 0, 'n\_estimators': 10, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 2, 'random\_state': 0, 'n\_estimators': 18, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.96      1.00      0.98      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6334   80]
 [   6 1723]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1639   54]
 [   4  968]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      7703
          1       0.96      1.00      0.98      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7616   87]
 [  10 2039]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 3, 'random\_state': 0, 'n\_estimators': 10, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.98      0.99      6414
          1       0.92      0.98      0.95      1729

avg / total       0.98      0.98      0.98      8143


Train Confusion Matrix:
[[6270  144]
 [  34 1695]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.98      0.96      0.97      1693
          1       0.93      0.97      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1620   73]
 [  27  945]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.73      0.84      7703
          1       0.49      0.98      0.65      2049

avg / total       0.89      0.78      0.80      9752


Test2 Confusion Matrix:
[[5620 2083]
 [  38 2011]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'n\_estimators': 15, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.95      0.95      6414
          1       0.82      0.78      0.80      1729

avg / total       0.92      0.92      0.92      8143


Train Confusion Matrix:
[[6118  296]
 [ 381 1348]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.89      0.87      0.88      1693
          1       0.78      0.81      0.80       972

avg / total       0.85      0.85      0.85      2665


Test1 Confusion Matrix:
[[1469  224]
 [ 180  792]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.92      0.90      0.91      7703
          1       0.65      0.70      0.67      2049

avg / total       0.86      0.86      0.86      9752


Test2 Confusion Matrix:
[[6919  784]
 [ 622 1427]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'max\_depth': 3, 'random\_state': 0, 'n\_estimators': 14, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6328   86]
 [   3 1726]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1637   56]
 [   2  970]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      7703
          1       0.89      1.00      0.94      2049

avg / total       0.98      0.97      0.97      9752


Test2 Confusion Matrix:
[[7443  260]
 [   7 2042]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'max\_depth': 1, 'random\_state': 0, 'n\_estimators': 10, 'min\_samples\_split': 2\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.83      0.97      0.89      6414
          1       0.70      0.25      0.37      1729

avg / total       0.80      0.82      0.78      8143


Train Confusion Matrix:
[[6231  183]
 [1296  433]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.69      1.00      0.81      1693
          1       1.00      0.21      0.35       972

avg / total       0.80      0.71      0.64      2665


Test1 Confusion Matrix:
[[1692    1]
 [ 768  204]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.82      0.97      0.89      7703
          1       0.64      0.20      0.31      2049

avg / total       0.78      0.81      0.77      9752


Test2 Confusion Matrix:
[[7472  231]
 [1637  412]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.25\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright
Hyper Paramters\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Train\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test2\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`Weekend', `WorkingHour'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 2, `n\_estimators': 10,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.94\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 2, `n\_estimators': 18,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`WorkingHour', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 3, `n\_estimators': 10,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.89\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`CO2', `Temperature'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `n\_estimators': 15,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.92\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.85\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.86\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 3, `n\_estimators': 14,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.25\columnwidth}\raggedright
`Weekend', `HumidityRatio'\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `n\_estimators': 10,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.80\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.80\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.78\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    With random forest, after 40 minutes of crazy fan sounds getting out of
my laptop, the scenery seems to be same as the Decision Tree above. But,
interestingly Weekend-Humidity ratio gained much more accuracy than it
had in the Decision Tree.

    \hypertarget{gradient-boosting-machine}{%
\subsection{Gradient Boosting Machine}\label{gradient-boosting-machine}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingClassifier}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.08}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{120}\PY{p}{)}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
         
             \PY{n}{gbc} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingClassifier}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{gbc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{gbc}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{gbc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{gbc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{gbc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{}print(gbc.feature\PYZus{}importances\PYZus{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'learning\_rate': 0.1, 'n\_estimators': 100, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'learning\_rate': 0.08, 'n\_estimators': 112, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      1.00      6414
          1       0.97      1.00      0.98      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6359   55]
 [   1 1728]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.97      0.96      1693
          1       0.95      0.89      0.92       972

avg / total       0.94      0.94      0.94      2665


Test1 Confusion Matrix:
[[1644   49]
 [ 103  869]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.99      0.99      7703
          1       0.96      0.98      0.97      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7613   90]
 [  49 2000]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'learning\_rate': 0.01, 'n\_estimators': 100, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.98      0.99      6414
          1       0.92      0.98      0.95      1729

avg / total       0.98      0.98      0.98      8143


Train Confusion Matrix:
[[6273  141]
 [  42 1687]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.98      0.96      0.97      1693
          1       0.93      0.97      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1620   73]
 [  32  940]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.73      0.84      7703
          1       0.49      0.98      0.65      2049

avg / total       0.89      0.78      0.80      9752


Test2 Confusion Matrix:
[[5622 2081]
 [  38 2011]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'learning\_rate': 0.01, 'n\_estimators': 111, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.95      0.97      0.96      6414
          1       0.88      0.80      0.83      1729

avg / total       0.93      0.93      0.93      8143


Train Confusion Matrix:
[[6221  193]
 [ 354 1375]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.70      0.91      0.79      1693
          1       0.67      0.33      0.44       972

avg / total       0.69      0.70      0.66      2665


Test1 Confusion Matrix:
[[1537  156]
 [ 652  320]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.88      0.72      0.79      7703
          1       0.37      0.62      0.46      2049

avg / total       0.77      0.70      0.72      9752


Test2 Confusion Matrix:
[[5524 2179]
 [ 781 1268]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'learning\_rate': 0.1, 'n\_estimators': 110, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      1.00      6414
          1       0.97      1.00      0.98      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6362   52]
 [   1 1728]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.97      0.96      1693
          1       0.95      0.89      0.92       972

avg / total       0.94      0.94      0.94      2665


Test1 Confusion Matrix:
[[1645   48]
 [ 103  869]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.99      0.99      7703
          1       0.95      0.97      0.96      2049

avg / total       0.98      0.98      0.98      9752


Test2 Confusion Matrix:
[[7597  106]
 [  68 1981]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'learning\_rate': 0.01, 'n\_estimators': 105, 'random\_state': 0\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.80      1.00      0.89      6414
          1       0.96      0.09      0.16      1729

avg / total       0.84      0.81      0.74      8143


Train Confusion Matrix:
[[6408    6]
 [1573  156]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.64      1.00      0.78      1693
          1       0.00      0.00      0.00       972

avg / total       0.40      0.64      0.49      2665


Test1 Confusion Matrix:
[[1693    0]
 [ 972    0]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      7703
          1       0.00      0.00      0.00      2049

avg / total       0.62      0.79      0.70      9752


Test2 Confusion Matrix:
[[7703    0]
 [2049    0]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.28\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.43\columnwidth}\raggedright
Hyper Paramters\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Train\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test1\strut
\end{minipage} & \begin{minipage}[b]{0.05\columnwidth}\raggedright
Test2\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`Weekend', `WorkingHour'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 100, `learning\_rate':
0.1\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.94\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 112, `learning\_rate':
0.08\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.94\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`WorkingHour', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 100, `learning\_rate':
0.01\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.89\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`CO2', `Temperature'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 111, `learning\_rate':
0.01\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.93\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.69\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.77\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 110, `learning\_rate':
0.1\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.94\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.98\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.28\columnwidth}\raggedright
`Weekend', `HumidityRatio'\strut
\end{minipage} & \begin{minipage}[t]{0.43\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 105, `learning\_rate':
0.01\}\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.84\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.40\strut
\end{minipage} & \begin{minipage}[t]{0.05\columnwidth}\raggedright
0.62\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    Again, no significant change. But this model together with random
forest, reinforces the accuracy values achieved by Decision Tree.

In this case Weekend-Humidity ratio appears to be overfitted.

    \hypertarget{kernelized-svm}{%
\subsection{Kernelized SVM}\label{kernelized-svm}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
         
         \PY{n}{hyper\PYZus{}params\PYZus{}space} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{p}{\PYZcb{}}\PY{p}{,}
         \PY{p}{]}
         
         \PY{k}{for} \PY{n}{features} \PY{o+ow}{in} \PY{n}{features\PYZus{}combs\PYZus{}list}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{features}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===================================}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t1} \PY{o}{=} \PY{n}{X\PYZus{}test1}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
             \PY{n}{X\PYZus{}t2} \PY{o}{=} \PY{n}{X\PYZus{}test2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}
         
             \PY{n}{svc} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{hyper\PYZus{}params\PYZus{}space}\PY{p}{,}
                                \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{svc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Best parameters set:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{svc}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}
                 \PY{p}{(}\PY{n}{svc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{svc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                 \PY{p}{(}\PY{n}{svc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}t2}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{p}{]}
             
             \PY{k}{for} \PY{n}{pred} \PY{o+ow}{in} \PY{n}{preds}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Classification Report:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ Confusion Matrix:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
('Weekend', 'WorkingHour')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.95      0.97      6414
          1       0.84      0.99      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6096  318]
 [  20 1709]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.95      0.97      1693
          1       0.91      0.98      0.95       972

avg / total       0.96      0.96      0.96      2665


Test1 Confusion Matrix:
[[1602   91]
 [  16  956]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.99      0.89      0.94      7703
          1       0.71      0.98      0.82      2049

avg / total       0.94      0.91      0.92      9752


Test2 Confusion Matrix:
[[6887  816]
 [  38 2011]]

('Light', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6324   90]
 [   5 1724]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.97      0.98      1693
          1       0.95      1.00      0.97       972

avg / total       0.98      0.98      0.98      2665


Test1 Confusion Matrix:
[[1638   55]
 [   3  969]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      7703
          1       0.97      1.00      0.98      2049

avg / total       0.99      0.99      0.99      9752


Test2 Confusion Matrix:
[[7633   70]
 [  10 2039]]

('WorkingHour', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.98      0.97      0.98      6414
          1       0.91      0.91      0.91      1729

avg / total       0.96      0.96      0.96      8143


Train Confusion Matrix:
[[6252  162]
 [ 153 1576]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.95      0.96      0.96      1693
          1       0.93      0.92      0.92       972

avg / total       0.94      0.94      0.94      2665


Test1 Confusion Matrix:
[[1625   68]
 [  81  891]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.85      0.89      7703
          1       0.57      0.79      0.66      2049

avg / total       0.86      0.83      0.84      9752


Test2 Confusion Matrix:
[[6510 1193]
 [ 435 1614]]

('CO2', 'Temperature')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.96      0.94      0.95      6414
          1       0.79      0.84      0.81      1729

avg / total       0.92      0.92      0.92      8143


Train Confusion Matrix:
[[6032  382]
 [ 280 1449]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.91      0.87      0.89      1693
          1       0.79      0.85      0.82       972

avg / total       0.86      0.86      0.86      2665


Test1 Confusion Matrix:
[[1467  226]
 [ 146  826]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.94      0.75      0.83      7703
          1       0.46      0.81      0.59      2049

avg / total       0.84      0.76      0.78      9752


Test2 Confusion Matrix:
[[5788 1915]
 [ 392 1657]]

('Weekend', 'WorkingHour', 'Light', 'CO2')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.99      0.99      6414
          1       0.95      1.00      0.97      1729

avg / total       0.99      0.99      0.99      8143


Train Confusion Matrix:
[[6323   91]
 [   3 1726]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.96      0.98      1693
          1       0.94      1.00      0.97       972

avg / total       0.98      0.97      0.97      2665


Test1 Confusion Matrix:
[[1627   66]
 [   3  969]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       1.00      0.98      0.99      7703
          1       0.92      0.99      0.95      2049

avg / total       0.98      0.98      0.98      9752


Test2 Confusion Matrix:
[[7521  182]
 [  28 2021]]

('Weekend', 'HumidityRatio')
===================================
Best parameters set:
\{'random\_state': 0, 'kernel': 'linear'\}

Train Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      6414
          1       0.00      0.00      0.00      1729

avg / total       0.62      0.79      0.69      8143


Train Confusion Matrix:
[[6414    0]
 [1729    0]]

Test1 Classification Report:

             precision    recall  f1-score   support

          0       0.64      1.00      0.78      1693
          1       0.00      0.00      0.00       972

avg / total       0.40      0.64      0.49      2665


Test1 Confusion Matrix:
[[1693    0]
 [ 972    0]]

Test2 Classification Report:

             precision    recall  f1-score   support

          0       0.79      1.00      0.88      7703
          1       0.00      0.00      0.00      2049

avg / total       0.62      0.79      0.70      9752


Test2 Confusion Matrix:
[[7703    0]
 [2049    0]]


    \end{Verbatim}

    \begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright
Hyper Paramters\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Train\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Test1\strut
\end{minipage} & \begin{minipage}[b]{0.06\columnwidth}\raggedright
Test2\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`Weekend', `WorkingHour'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.94\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`WorkingHour', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.96\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.94\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.86\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`CO2', `Temperature'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.92\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.86\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.84\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.98\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
`Weekend', `HumidityRatio'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.62\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.40\strut
\end{minipage} & \begin{minipage}[t]{0.06\columnwidth}\raggedright
0.62\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    Firstly, all kernel parameters resulted as ``linear''. Light-CO2 seems
to hitted a good accuracy but, I believe Weekend-WorkingHour-Light-CO2
is more reliable. Because the former may had become underfit.

    \hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

    \begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.11\columnwidth}\raggedright
Model\strut
\end{minipage} & \begin{minipage}[b]{0.17\columnwidth}\raggedright
Features\strut
\end{minipage} & \begin{minipage}[b]{0.33\columnwidth}\raggedright
Parameters\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\raggedright
Training Accuracy\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedright
Test1 Accuracy\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedright
Test2 Accuracy\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Logistic Regression\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `C': 1.5, `penalty': `l1'\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Nave Bayes\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.97\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
K-Nearest Neighbors\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`n\_neighbors': 33\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.97\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Decision Tree\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 1, `random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Random Forest\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`min\_samples\_split': 2, `max\_depth': 3, `n\_estimators': 14,
`random\_state': 0\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Gradient Boosting Machine\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Weekend', `WorkingHour', `Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `n\_estimators': 110, `learning\_rate':
0.1\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.94\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
Kernelized SVM\strut
\end{minipage} & \begin{minipage}[t]{0.17\columnwidth}\raggedright
`Light', `CO2'\strut
\end{minipage} & \begin{minipage}[t]{0.33\columnwidth}\raggedright
\{`random\_state': 0, `kernel': `linear'\}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
0.99\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.98\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright
0.99\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    All in all, all models in general did a great job mostly using Light-CO2
alone. Only in some rare circumstances, my features was of help, little
they may be though.

(In fact, this table is scaring me. I cannot be sure whether I have
interpreted the results correctly or not.)


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
